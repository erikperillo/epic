\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Efficient Visual Attention with Deep Learning}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Visual saliency is an important component of attention.
It helps animals survive and can also be used by computer vision applications
to filter out irrelevant information from high volumes of data.
In this work, we present a new fully convolutional neural network
architecture designed for detecting visual saliency.
We also propose methods of data pre-processing that are specifically
benefical for the task of visual saliency detection.
Experiments carried out with the MIT300 benchmark presented state-of-the-art
performance and a parameter reduction of 3/4 compared to similar models.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
One of the most challenging unsolved problems in Artificial Intelligence
is vision.
However, it is fundamental for the conception of systems that interact
with the real physical world.
Such systems would be useful for applications in areas like
domestic services, industry and agriculture,
with great potential for the benefit of society.

Vision is remarkably data and computationally intensive.
In humans, approximately half of the brain is involved in
vision-related tasks~\cite{fixott_1957}.
Even our minds can not handle all the sheer amount of sensorial information
received every second. In order to deal with this amount of data,
humans have attentional systems, a fundamental mechanism
that, among other functions, filters out irrelevant information
-- either visual or from other senses-- and helps us focusing our cognitive
processes on what is important at a given moment.
These facts are a strong evidence that, in order to help solving the
vision problems, attention should be applied.

Visual attention can be defined as the delimitation of a certain spatial
region on an image for further cognitive processing~\cite{treisman_1980}.
The phenomenon emerges from two fundamentally different processes:
the \emph{top-down} mechanism that implements our longer-term cognitive
strategies by biasing attention according to one's interests
(e.g. find a red apple in a tree because of hunger,
which will make red be more recognizable on the scene),
and the \emph{bottom-up} mechanism~\cite{colombini_2016},
a process generated through external stimuli
that captures one's attention from its conspicuousness level.
In this work, we focus on the latter, also named visual saliency.

Visually salient regions on images are usually represented by
\emph{saliency maps} (Figure~\ref{fig:example}). In these maps, images are generated such that
areas with high-valued pixels express high saliency on the original image,
whereas regions with low-valued pixels represent low saliency.
Datasets with such maps are obtained by collecting eye-fixation
data from humans while observing the scenes.

\begin{center}
\begin{figure}[t]
\begin{tabular} {cc}
\includegraphics[width=0.22\textwidth]{./img/traffic_sign_s.jpg} &
\includegraphics[width=0.22\textwidth]{./img/traffic_sign_m.jpg}\\
(a) & (b)
\end{tabular}
\caption{Example of visual saliency.
    b) is the saliency map where brighter pixels (warmer colors)
    represent regions more salient to humans on the original image a).}
\label{fig:example}
\end{figure}
\end{center}

\subsection{Related work}
Early computational models of visual saliency were generally built based on
filtering of images for extraction of a pre-selected set of features
considered important for \emph{bottom-up} attention.
\emph{Vocus}~\cite{frintrop_2005} is a computational model that extracts
features shown to be naturally salient to humans such as color/luminance
contrast and orientation from different scales of the image.

A rapid change of paradigm occurred around 2015 when \emph{Deep Learning}
techniques showed to be very effective in the generation of saliency
maps.
\emph{Salicon}~\cite{jiang_2015} demonstrated that the use of
convolutional neural networks with weights initialized from
image classification networks, e.g. \emph{VGG-16}~\cite{zisserman_2014}
could considerably increase the similarity of computed maps to those
generated from humans.
\emph{Salicon} uses different scales of the image as input to capture relevant
information in the context of saliency, using the same network weights
for each dimension.
\emph{ML-Net}~\cite{cornia_2016} uses the output of different layers
of \emph{VGG-16}, combining them in many dimensions and various levels of
abstraction.
\emph{DeepFix}~\cite{kruthiventi_2015} extends a pre-trained model with
inception~\cite{szegedy_2014} layers -- which use information from different
scales of the image -- and a component for center bias, a
phenomenon that arises from our tendency to take pictures with relevant
objects at the center of the image.
\emph{Salnet}~\cite{pan_2016} explores two models: a shallow convolutional
network followed by a fully-connected layer and a fully convolutional
neural network with first layers' weights initialized from \emph{VGG-16}.

Visual saliency detection models are usually evaluated and ranked on
\emph{MIT saliency benchmark}~\cite{mit_sal_bm}, which uses a variety of
metrics to express how close generated saliency maps are to those created
from human data.

\subsection{Motivation}
Current state of the art models are in general quite expensive computationally,
partly because most of them are based on big pre-trained networks.
The convolutional layers of \emph{VGG-16} are composed of around 14.7
million parameters.
While pre-trained weights from classification tasks showed to be effective
for saliency prediction, it is reasonable to question whether
creating a proper network from scratch could yield a smaller amount of
parameters that are more efficient for the sole task of saliency prediction.
Also, there are some data processing methods from previous work on
psychology-based models that were not used in current models but
are considered worthwhile to explore,
such as using global information from the scene and a colorspace more
closely related to human vision.

\subsection{Objectives}
This work aims at building a visual saliency model that is a) effective,
yielding results similar to other state of the art models,
and b) relatively simple and computationally efficient.
It is important that both criteria are matched in order to extend the
model in the future for video and real time computer vision applications
such as navigating robots.

\section{Proposed model}
Figure~\ref{fig:model} shows the overall architecture of the fully
convolutional neural network proposed in this work.
It extracts features from increasingly smaller dimensions of the
input image.
The network is composed of four main blocks:

\begin{enumerate}
    \item The first level extracts low level features from the input image, of
        dimensions $W\times H \times 3$ (width, height, depth), using
        a single layer with 48 convolution filters with ReLu activation
        followed by max-pooling that reduces image by a factor of two.
        It was found that further decreasing the number of filters in this
        layer considerably hurts performance, which makes sense because it is
        important to capture high spatial frequency and high contrast
        information in the context of visual saliency.
    \item The second level extracts low-medium level features from the
        input of dimensions $W/2 \times H/2 \times 48$ using two layers
        with 74 and 96 convolution filters, respectively, followed by ReLU
        activation and max-pooling.
    \item The third level extracts medium-high level features from input with
        dimensions $W/4 \times H/4 \times 96$ using four convolution layers
        in sequence with 128, 144, 160 and 172 filters.
        Every convolution layer is followed by ReLu.
        Max-pooling is carried out at the end.
        A considerable depth in this level was found to be important for
        the network's performance.
    \item The fourth and last level is composed of eight inception blocks
        that extract high level features from the input with
        dimensions $W/8 \times H/8 \times 172$.
        Great level of depth and Inception blocks were found to be very
        important at this level.
        A $1 \times 1$ convolution makes a linear combination of the output
        maps at the end of the 8 inception blocks, followed by ReLu, producing
        the final saliency map of dimensions $W/8 \times H/8 \times 1$.
        The saliency map is then resized to the original dimensions using
        bicubic interpolation.
\end{enumerate}

\begin{figure*}
\begin{center}
\def\svgwidth{1.5\columnwidth}
\input{./img/model.pdf_tex}
\label{fig:model}
    \caption{Overview of the network.
        Filters sizes are in format width$\times$height\_stride.}
\end{center}
\end{figure*}

Figure \ref{fig:newinception} illustrates the inception
architecture used.
Each inception layer applies
$3 \times 3$ convolution, a sequence of two $3 \times 3$ convolutions
(these two types are preceded by $1\times 1$ convolutions in order to
reduce the number of input filters), $1 \times 1$ convolution,
and a max-pooling of size $3 \times 3$.
Each of those operations is executed in parallel from the same input
and the outputs are concatenated at the output.
Inception allows the network to use
information from different spatial dimensions as well as previous
layers (lower level saliency information) in the final map
computation, which is considered to be important for visual saliency.
The network has a total of $3593842$ parameters, a very low number
compared to other models.
Table \ref{table:inception} details the filter configuration for the
inception layers.

\begin{figure}[!htb]
    \centering
    \def\svgwidth{\linewidth}
    \input{./img/inception.pdf_tex}
    \caption{Inception block layout.}
   \label{fig:newinception}
\end{figure}

\begin{table*}
\begin{center}
\small
\label{table:inception}
\caption{Number of filters used in each inception block.}
\begin{tabular}{|c|c|c|c|c|c|c|}
	\hline
    Block & pool & 1$\times$1 & 3$\times$3 reduce &
    3$\times$3 & double 3$\times$3 reduce & double 3$\times$3\\
    \hline
    1 & 64 & 128 & 80 & 160 & 24 & 48\\
    \hline
    2 & 64 & 128 & 80 & 160 & 24 & 48\\
    \hline
    3 & 64 & 128 & 80 & 160 & 24 & 48\\
    \hline
    4 & 64 & 128 & 96 & 192 & 28 & 56\\
    \hline
    5 & 64 & 128 & 96 & 192 & 28 & 56\\
    \hline
    6 & 64 & 128 & 112 & 224 & 32 & 64\\
    \hline
    7 & 64 & 128 & 112 & 224 & 32 & 64\\
    \hline
    8 & 112 & 160 & 128 & 256 & 40 & 80\\
    \hline
\end{tabular}
\end{center}
\end{table*}

\subsection{Data pre-processing}
Input images were resized to dimensions $320\times240\times3$.
Each image is normalized channel-wise by the subtraction of
the channel mean and division by the standard deviation:
$$C = \frac{C - \mu_C}{\sigma_C}$$
Most models used for comparison and cited in this work use RGB images
normalized channel-wise using statistics computed from the dataset.
However, visual saliency is highly connected to the context of the
image, hence saliency depends on the local context.

Images are converted from RGB colorspace to the LAB colorspace.
\emph{Vocus}~\cite{frintrop_2005} cites that the
LAB colorspace is more closely related to human vision once it encompasses
red-green, yellow-blue and
luminance maps.

Section~\ref{sec:dataproc_compar} describes experiments carried out
to investigate how different data pre-processing techniques affect the
performance of the model.

\subsection{Implementation}
The network was implemented using \emph{Tensorflow} \texttt{1.3.0}
on a machine with \emph{Ubuntu 16.04 LTS} and
kernel \emph{Linux} \texttt{4.8.0-54-generic}.
Training was conducted on a GPU \emph{NVIDIA GTX 1080} and the
code is available at \texttt{https://goo.gl/WzpyYJ}.

\subsection{Training}
Two datasets were used:
\emph{SALICON}~\cite{jiang_2015}, with 15000 images, and
\emph{Judd}~\cite{judd}, with 1003 images.
Data augmentation was applied by flipping images
horizontally and vertically and applying random disturbances to the image:
blur, adding noise, shifting and shearing.
The cost function to be minimized was the \emph{Mean Squared Error} between
the ground-truth saliency map $G$ and the predicted map $P$, using
the \emph{Adam Optimizer}.

Training iterated calculating the mean loss on the validation set
after each epoch, and it stopped when the validation loss started to increase.
\emph{SALICON} dataset was first used.
Number of images was $12000$ in the training and $1500$ in the validation set.
Training iterated for 12 epochs with learning rate of $1\times10^{-4}$
and batch size of 24
Finally, \emph{Judd} was used, with $800$ images in the training set
and $203$ images in the validation set.
Training iterated for 3 epochs with learning rate of $1\times10^{-5}$
and batch size of 10.
The whole training process took around 4 hours.

\section{Experimental Results}
\begin{figure*}
\begin{center}
    \begin{tabular} {ccc}
    Stimulus & Ground-truth & Proposed Model\\
    \includegraphics[width=0.25\textwidth]{./img/monkey_s.jpg} &
    \includegraphics[width=0.25\textwidth]{./img/monkey_gt.jpg} &
    \includegraphics[width=0.25\textwidth]{./img/monkey_m.jpg}\\
    \includegraphics[width=0.25\textwidth]{./img/person_s.jpg} &
    \includegraphics[width=0.25\textwidth]{./img/person_gt.jpg} &
    \includegraphics[width=0.25\textwidth]{./img/person_m.jpg}\\
    \includegraphics[width=0.25\textwidth]{./img/sign_s.jpg} &
    \includegraphics[width=0.25\textwidth]{./img/sign_gt.jpg} &
    \includegraphics[width=0.25\textwidth]{./img/sign_m.jpg}\\
    \end{tabular}
\end{center}
    \caption{Examples of predictions made by the proposed model.}
    \label{fig:preds}
\end{figure*}

\begin{table*}
	\small
    \begin{center}
    \label{table:results}
    \caption{State of the art models and metric scores on
    \emph{MIT300 benchmark}.}
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        Model & Num. parameters & AUC-Judd $\uparrow$ & CC $\uparrow$
            & NSS $\uparrow$ & Sim $\uparrow$ & EMD $\downarrow$\\
        \hline
        Baseline: Infinite humans & - & 0.92 & 1.0 & 3.29 & 1.0 & 0\\
        \hline
        \emph{DeepFix} & $\approx$16.7 million & 0.87 & 0.78
            & 2.26 & 0.67 & 2.04\\
        \hline
        \emph{Salicon} & $\approx$14.7 million & 0.87 & 0.74 & 2.12
            & 0.60 & 2.62\\
        \hline
        \textbf{Proposed Model} & $\approx$\textbf{3.6 million}
            & \textbf{0.85} &
        \textbf{0.71} & \textbf{1.98} & \textbf{0.62} & \textbf{2.37}\\
        \hline
        \emph{ML-Net} & $\approx$15.4 million & 0.85 & 0.69 & 2.07 & 0.60
            & 2.53\\
        \hline
        \emph{SalNet} & $\approx$25.8 million & 0.83 & 0.57 & 1.51
            & 0.52 & 3.31\\
        \hline
    \end{tabular}
    \end{center}
\end{table*}

\subsection{Results on MIT300 benchmark}
Prediction took an average time of 10 milliseconds.
Figure~\ref{fig:preds} shows some maps generated by the proposed model.
They are generally considerably similar to the ground truth. For evaluation, we submitted the model to the \emph{MIT300 saliency benchmark} that has around 300 images and is commonly used to rank such models. Table~\ref{table:results} shows the resulting values for the most common metrics.
The proposed model achieved results comparable to those of the state of the art while having, at least, one fourth of the number of parameters.

\subsection{Comparison of data processing methods}
\label{sec:dataproc_compar}
In order to investigate how the different data pre-processing methods
affected model's performance, a comparison was made using two configurations:
1) Using LAB colorspace and normalizing channel-wise with mean and standard
deviation per image; and 2) using RGB colorspace, normalizing channel-wise
with mean and standard deviation of the training dataset.
The latter is commonly used in other models cited in this work.
For each configuration, the model was trained on the
\emph{SALICON} dataset, which as split into
12000, 1500 and 1500 samples for training, validation and test, respectively.

Figure~\ref{fig:confs_losses} shows that configuration 1) converged faster
and to a better optimum value.
Figure~\ref{fig:confs_metrics} shows metrics computations on the test set.
Overall, configuration 1) was better.
We conjecture that the LAB colorspace in combination with normalization per
image facilitates extraction of importante features for the context
of saliency: luminance and color contrasts and information on the global level
of the image.

\begin{figure*}
\begin{center}
\begin{tabular} {cc}
\includegraphics[width=0.45\linewidth]{./img/lab_perimg_losses.png} &
\includegraphics[width=0.45\linewidth]{./img/rgb_perdset_losses.png}\\
(1) & (2)
\end{tabular}
\caption{Progression of losses on test and validation
    sets on configuration 1) (left) and 2) (right) during training.}
\label{fig:confs_losses}
\end{center}
\end{figure*}

\begin{center}
\begin{figure}[t]
\includegraphics[width=0.5\textwidth]{./img/metrics.png}
\caption{Metrics evaluation on the test set for both configurations.}
\label{fig:confs_metrics}
\end{figure}
\end{center}

\section{Conclusion}
In this paper, we proposed a novel fully convolutional neural network for the
prediction of visual saliency on images.
The proposed model architecture was designed specifically
for the task of saliency prediction.
Normalizing images channel-wise per image -- rather than per dataset -- showed
to be important for a good performance of the model.
Our methods showed to be effective, yielding to a network with performance
on \emph{MIT300 benchmark} consistently among the ten best results on various
metrics while having around $3/4$  less parameters than other state of
the art models.

{\small
\bibliographystyle{ieee}
\bibliography{bibliography}
}

\end{document}
